{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rookiehhh/AI-NoteBooks/blob/main/llm/pruning_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aGGUT9V-qxOa"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "Jsq8Txdl8SeG",
        "outputId": "2ebe11b6-e1eb-444e-ae37-ba3440e1c878",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Dec 16 00:50:41 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P0              28W /  70W |    105MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 查看GPU\n",
        "print(torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "id": "ROB0G4RN73sz",
        "outputId": "5d58cbc9-25e3-41ea-8a95-6b3b28e00c5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYkm6713qxOd"
      },
      "source": [
        "剪枝教程\n",
        "================\n",
        "\n",
        "**作者**: [Michela Paganini](https://github.com/mickypaganini)\n",
        "\n",
        "最先进的深度学习技术依赖于过于参数化的模型，这些模型难以部署。相反，生物神经网络以高效的稀疏连接而闻名。找到通过减少模型中参数数量来压缩模型的最佳技术非常重要，以减少内存、电池，并且……稀疏化你的神经网络，以及如何扩展它以实现你自己的自定义剪枝技术。\n",
        "\n",
        "要求\n",
        "------------\n",
        "\n",
        "`\"torch>=1.4.0a0+8e8a5e0\"`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MkKbhEqKqxOe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw1bIloFqxOf"
      },
      "source": [
        "创建模型\n",
        "==============\n",
        "\n",
        "在本教程中，我们使用了\n",
        "[LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf) 架构\n",
        "来自 LeCun 等人, 1998 年。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wdihXHh-qxOf"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # 检查是否可以使用CUDA，如果不能则使用CPU\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        # 1个输入图像通道，6个输出通道，5x5的卷积核\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 图像尺寸为5x5\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)  # 最后一层分类，输出10类\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 应用第一个卷积层，之后接ReLU激活函数和2x2的最大池化层\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        # 应用第二个卷积层，之后接ReLU激活函数和2x2的最大池化层\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
        "        # 展平特征图为一维\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        # 应用第一个全连接层\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # 应用第二个全连接层\n",
        "        x = F.relu(self.fc2(x))\n",
        "        # 应用第三个全连接层，输出分类结果\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        # 计算张量x中非batch维数的元素总数，用于展平操作\n",
        "        size = x.size()[1:]  # 除去batch维度的其他维度\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "# 实例化模型并将其移动到设备上（GPU或CPU）\n",
        "model = LeNet().to(device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwXAgF8jqxOf"
      },
      "source": [
        "检查一个模块\n",
        "================\n",
        "\n",
        "让我们检查一下我们 LeNet 模型中的 (未剪枝的) `conv1` 层。它将包含两个参数 `weight` 和 `bias`，目前没有缓冲区。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "34MbeKCQqxOf",
        "outputId": "b9cbc8d3-36aa-46b5-d8aa-39be4f94d302",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('weight', Parameter containing:\n",
            "tensor([[[[ 5.3230e-02, -1.9184e-01, -1.5453e-01, -1.2699e-01,  6.6093e-02],\n",
            "          [ 1.8400e-01, -9.0329e-02, -1.2064e-01, -1.8656e-01, -1.7463e-01],\n",
            "          [ 7.2806e-02, -1.5050e-01, -7.3161e-02, -1.1542e-01, -1.0708e-01],\n",
            "          [ 2.7203e-02, -1.6833e-01, -6.0692e-02, -4.8347e-02, -3.6435e-02],\n",
            "          [-1.1822e-01,  7.2270e-02,  1.9358e-01, -1.2367e-01, -1.2044e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.1181e-01, -3.6317e-03, -3.7799e-02, -2.1182e-02, -1.9294e-01],\n",
            "          [-9.4081e-02, -1.6163e-01, -1.7939e-01, -1.9383e-02,  1.7369e-01],\n",
            "          [ 1.8170e-01,  6.0109e-02, -2.9164e-02,  1.2290e-01,  4.4267e-02],\n",
            "          [-6.1668e-02, -1.8145e-01, -1.8272e-01, -4.7278e-02, -2.5070e-02],\n",
            "          [-1.5373e-01,  8.0089e-02,  2.5887e-02,  6.4049e-02,  7.2076e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.7609e-02, -1.3780e-01, -5.4621e-02, -1.2206e-02,  5.2841e-02],\n",
            "          [-5.2555e-02, -1.8912e-01,  6.6217e-02,  2.7229e-02,  2.3559e-03],\n",
            "          [-5.6309e-02, -1.8930e-01,  9.6571e-02, -4.4901e-02, -1.5537e-02],\n",
            "          [ 7.9977e-02, -5.8682e-02, -1.6047e-02, -1.0605e-01,  7.9221e-02],\n",
            "          [-1.6149e-01,  6.6729e-02, -2.9748e-02, -7.2269e-02, -1.1112e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 8.5882e-02,  1.0928e-01, -1.1594e-01, -7.2526e-02, -1.9875e-01],\n",
            "          [ 7.2294e-02, -1.0453e-01,  6.0929e-03,  1.5808e-01,  4.5754e-02],\n",
            "          [-9.5228e-03,  3.0368e-02, -1.0535e-01,  1.2455e-01, -1.0528e-01],\n",
            "          [ 1.2656e-01,  1.0943e-05, -2.4319e-02,  1.9900e-01, -6.5929e-02],\n",
            "          [ 1.7295e-01, -4.3150e-02, -1.3977e-01, -1.1677e-01,  4.0837e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.8692e-02,  5.5041e-02, -4.8174e-02, -3.2240e-02, -6.6495e-03],\n",
            "          [-1.5058e-01,  2.0878e-02,  1.2877e-01, -4.8080e-02, -1.8864e-01],\n",
            "          [ 2.4116e-02,  2.9690e-03, -8.6705e-02,  1.0196e-01, -1.5521e-01],\n",
            "          [-1.2146e-01,  3.5081e-02, -1.8921e-01, -1.6350e-02, -9.8950e-02],\n",
            "          [-1.6135e-01, -1.3056e-01,  1.2149e-01,  3.3998e-02, -1.0888e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 3.1408e-02,  1.8401e-01, -1.4511e-01, -1.9848e-01, -1.9693e-01],\n",
            "          [-2.5736e-03, -1.9103e-01,  4.6310e-02, -5.2954e-02,  1.7836e-01],\n",
            "          [-1.6163e-01, -6.6080e-02,  1.8100e-01,  1.0677e-01,  1.4056e-01],\n",
            "          [-6.2171e-02, -1.6391e-01, -1.5709e-01, -9.8989e-02,  8.7996e-02],\n",
            "          [ 1.9245e-01,  4.7125e-02, -4.9759e-02,  1.1260e-01,  4.7297e-04]]]],\n",
            "       device='cuda:0', requires_grad=True)), ('bias', Parameter containing:\n",
            "tensor([-0.0082, -0.1869, -0.0693,  0.1968, -0.1167,  0.0080], device='cuda:0',\n",
            "       requires_grad=True))]\n"
          ]
        }
      ],
      "source": [
        "module = model.conv1\n",
        "print(list(module.named_parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bP5ooPGeqxOg",
        "outputId": "d2159a0d-2667-4474-8fa3-8e3a9f54850a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "print(list(module.named_buffers()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnA6AJIMqxOg"
      },
      "source": [
        "修剪模块\n",
        "================\n",
        "\n",
        "要修剪一个模块（在这个例子中，是我们 LeNet 架构的 `conv1` 层），首先从 `torch.nn.utils.prune` 中选择一种可用的修剪技术（或通过继承 `BasePruningMethod` [实现](#extending-torch-nn-utils-pruning-with-custom-pruning-functions) 自己的修剪方法）。然后，指定要修剪的模块及其内部参数的名称。最后，使用所选修剪技术所需的适当关键字参数，指定修剪参数。\n",
        "\n",
        "在这个例子中，我们将随机修剪 `conv1` 层中名为 `weight` 的参数中的 30% 连接。模块作为函数的第一个参数传递；`name` 通过其字符串标识符识别该模块中的参数；而 `amount` 表示要修剪的连接的百分比（如果是介于 0 和 1 之间的浮点数），或者要修剪的绝对连接数（如果是一个非负整数）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ytvne9C8qxOg",
        "outputId": "ad87dcf8-9afe-45bb-93fe-812173312fab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "prune.random_unstructured(module, name=\"weight\", amount=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byEQFyrFqxOh"
      },
      "source": [
        "修剪通过从参数中移除 `weight` 并用一个新的参数 `weight_orig` 替换它（即在初始参数 `name` 后附加 `\"_orig\"`）。 `weight_orig` 存储张量的未修剪版本。 `bias` 没有被修剪，因此将保持不变。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "fFlziTGmqxOh",
        "outputId": "3f2d0ec4-5d90-48b1-cfc2-d19627fa06ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('bias', Parameter containing:\n",
            "tensor([-0.0082, -0.1869, -0.0693,  0.1968, -0.1167,  0.0080], device='cuda:0',\n",
            "       requires_grad=True)), ('weight_orig', Parameter containing:\n",
            "tensor([[[[ 5.3230e-02, -1.9184e-01, -1.5453e-01, -1.2699e-01,  6.6093e-02],\n",
            "          [ 1.8400e-01, -9.0329e-02, -1.2064e-01, -1.8656e-01, -1.7463e-01],\n",
            "          [ 7.2806e-02, -1.5050e-01, -7.3161e-02, -1.1542e-01, -1.0708e-01],\n",
            "          [ 2.7203e-02, -1.6833e-01, -6.0692e-02, -4.8347e-02, -3.6435e-02],\n",
            "          [-1.1822e-01,  7.2270e-02,  1.9358e-01, -1.2367e-01, -1.2044e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.1181e-01, -3.6317e-03, -3.7799e-02, -2.1182e-02, -1.9294e-01],\n",
            "          [-9.4081e-02, -1.6163e-01, -1.7939e-01, -1.9383e-02,  1.7369e-01],\n",
            "          [ 1.8170e-01,  6.0109e-02, -2.9164e-02,  1.2290e-01,  4.4267e-02],\n",
            "          [-6.1668e-02, -1.8145e-01, -1.8272e-01, -4.7278e-02, -2.5070e-02],\n",
            "          [-1.5373e-01,  8.0089e-02,  2.5887e-02,  6.4049e-02,  7.2076e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.7609e-02, -1.3780e-01, -5.4621e-02, -1.2206e-02,  5.2841e-02],\n",
            "          [-5.2555e-02, -1.8912e-01,  6.6217e-02,  2.7229e-02,  2.3559e-03],\n",
            "          [-5.6309e-02, -1.8930e-01,  9.6571e-02, -4.4901e-02, -1.5537e-02],\n",
            "          [ 7.9977e-02, -5.8682e-02, -1.6047e-02, -1.0605e-01,  7.9221e-02],\n",
            "          [-1.6149e-01,  6.6729e-02, -2.9748e-02, -7.2269e-02, -1.1112e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 8.5882e-02,  1.0928e-01, -1.1594e-01, -7.2526e-02, -1.9875e-01],\n",
            "          [ 7.2294e-02, -1.0453e-01,  6.0929e-03,  1.5808e-01,  4.5754e-02],\n",
            "          [-9.5228e-03,  3.0368e-02, -1.0535e-01,  1.2455e-01, -1.0528e-01],\n",
            "          [ 1.2656e-01,  1.0943e-05, -2.4319e-02,  1.9900e-01, -6.5929e-02],\n",
            "          [ 1.7295e-01, -4.3150e-02, -1.3977e-01, -1.1677e-01,  4.0837e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.8692e-02,  5.5041e-02, -4.8174e-02, -3.2240e-02, -6.6495e-03],\n",
            "          [-1.5058e-01,  2.0878e-02,  1.2877e-01, -4.8080e-02, -1.8864e-01],\n",
            "          [ 2.4116e-02,  2.9690e-03, -8.6705e-02,  1.0196e-01, -1.5521e-01],\n",
            "          [-1.2146e-01,  3.5081e-02, -1.8921e-01, -1.6350e-02, -9.8950e-02],\n",
            "          [-1.6135e-01, -1.3056e-01,  1.2149e-01,  3.3998e-02, -1.0888e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 3.1408e-02,  1.8401e-01, -1.4511e-01, -1.9848e-01, -1.9693e-01],\n",
            "          [-2.5736e-03, -1.9103e-01,  4.6310e-02, -5.2954e-02,  1.7836e-01],\n",
            "          [-1.6163e-01, -6.6080e-02,  1.8100e-01,  1.0677e-01,  1.4056e-01],\n",
            "          [-6.2171e-02, -1.6391e-01, -1.5709e-01, -9.8989e-02,  8.7996e-02],\n",
            "          [ 1.9245e-01,  4.7125e-02, -4.9759e-02,  1.1260e-01,  4.7297e-04]]]],\n",
            "       device='cuda:0', requires_grad=True))]\n"
          ]
        }
      ],
      "source": [
        "print(list(module.named_parameters()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUUeR1f5qxOh"
      },
      "source": [
        "由上述选定的剪枝技术生成的剪枝掩码作为名为 `weight_mask` 的模块缓冲区保存（即在初始参数 `name` 后附加 `\"_mask\"`）。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "AmCiOpmlqxOh",
        "outputId": "d4299e9f-96ba-4d94-fc12-4289bf57819c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('weight_mask', tensor([[[[1., 0., 1., 1., 0.],\n",
            "          [1., 0., 1., 1., 0.],\n",
            "          [0., 1., 0., 1., 1.],\n",
            "          [1., 0., 0., 1., 1.],\n",
            "          [1., 0., 0., 0., 1.]]],\n",
            "\n",
            "\n",
            "        [[[1., 0., 1., 1., 1.],\n",
            "          [1., 1., 0., 1., 1.],\n",
            "          [0., 0., 1., 1., 0.],\n",
            "          [0., 0., 1., 1., 1.],\n",
            "          [0., 1., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[0., 1., 1., 1., 0.],\n",
            "          [1., 1., 1., 1., 1.],\n",
            "          [1., 0., 1., 0., 0.],\n",
            "          [1., 1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1., 0.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1., 1., 0., 1.],\n",
            "          [0., 1., 1., 0., 1.],\n",
            "          [0., 1., 0., 1., 0.],\n",
            "          [0., 1., 1., 0., 1.],\n",
            "          [1., 1., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1., 1., 1., 0.],\n",
            "          [1., 1., 1., 1., 0.],\n",
            "          [1., 1., 1., 1., 1.],\n",
            "          [0., 1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 1., 1., 0., 1.],\n",
            "          [1., 1., 0., 1., 1.],\n",
            "          [1., 0., 1., 1., 1.],\n",
            "          [1., 1., 1., 0., 1.],\n",
            "          [1., 0., 0., 1., 0.]]]], device='cuda:0'))]\n"
          ]
        }
      ],
      "source": [
        "print(list(module.named_buffers()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnJSPCqYqxOh"
      },
      "source": [
        "为了使前向传播正常工作而不需要修改，必须存在 `weight` 属性。 `torch.nn.utils.prune` 中实现的剪枝技术计算权重的剪枝版本（通过将掩码与原始参数结合）并将其存储在 `weight` 属性中。请注意，这不再是 `module` 的一个参数，它现在仅仅是一个属性。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "j4gr3khQqxOh",
        "outputId": "3b492e13-63fa-4653-8e47-0725dd1bd0d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[ 5.3230e-02, -0.0000e+00, -1.5453e-01, -1.2699e-01,  0.0000e+00],\n",
            "          [ 1.8400e-01, -0.0000e+00, -1.2064e-01, -1.8656e-01, -0.0000e+00],\n",
            "          [ 0.0000e+00, -1.5050e-01, -0.0000e+00, -1.1542e-01, -1.0708e-01],\n",
            "          [ 2.7203e-02, -0.0000e+00, -0.0000e+00, -4.8347e-02, -3.6435e-02],\n",
            "          [-1.1822e-01,  0.0000e+00,  0.0000e+00, -0.0000e+00, -1.2044e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.1181e-01, -0.0000e+00, -3.7799e-02, -2.1182e-02, -1.9294e-01],\n",
            "          [-9.4081e-02, -1.6163e-01, -0.0000e+00, -1.9383e-02,  1.7369e-01],\n",
            "          [ 0.0000e+00,  0.0000e+00, -2.9164e-02,  1.2290e-01,  0.0000e+00],\n",
            "          [-0.0000e+00, -0.0000e+00, -1.8272e-01, -4.7278e-02, -2.5070e-02],\n",
            "          [-0.0000e+00,  8.0089e-02,  2.5887e-02,  6.4049e-02,  7.2076e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000e+00, -1.3780e-01, -5.4621e-02, -1.2206e-02,  0.0000e+00],\n",
            "          [-5.2555e-02, -1.8912e-01,  6.6217e-02,  2.7229e-02,  2.3559e-03],\n",
            "          [-5.6309e-02, -0.0000e+00,  9.6571e-02, -0.0000e+00, -0.0000e+00],\n",
            "          [ 7.9977e-02, -5.8682e-02, -1.6047e-02, -1.0605e-01,  7.9221e-02],\n",
            "          [-1.6149e-01,  6.6729e-02, -2.9748e-02, -7.2269e-02, -0.0000e+00]]],\n",
            "\n",
            "\n",
            "        [[[ 8.5882e-02,  1.0928e-01, -1.1594e-01, -0.0000e+00, -1.9875e-01],\n",
            "          [ 0.0000e+00, -1.0453e-01,  6.0929e-03,  0.0000e+00,  4.5754e-02],\n",
            "          [-0.0000e+00,  3.0368e-02, -0.0000e+00,  1.2455e-01, -0.0000e+00],\n",
            "          [ 0.0000e+00,  1.0943e-05, -2.4319e-02,  0.0000e+00, -6.5929e-02],\n",
            "          [ 1.7295e-01, -4.3150e-02, -1.3977e-01, -1.1677e-01,  4.0837e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.8692e-02,  5.5041e-02, -4.8174e-02, -3.2240e-02, -0.0000e+00],\n",
            "          [-1.5058e-01,  2.0878e-02,  1.2877e-01, -4.8080e-02, -0.0000e+00],\n",
            "          [ 2.4116e-02,  2.9690e-03, -8.6705e-02,  1.0196e-01, -1.5521e-01],\n",
            "          [-0.0000e+00,  3.5081e-02, -1.8921e-01, -1.6350e-02, -9.8950e-02],\n",
            "          [-1.6135e-01, -1.3056e-01,  1.2149e-01,  3.3998e-02, -0.0000e+00]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000e+00,  1.8401e-01, -1.4511e-01, -0.0000e+00, -1.9693e-01],\n",
            "          [-2.5736e-03, -1.9103e-01,  0.0000e+00, -5.2954e-02,  1.7836e-01],\n",
            "          [-1.6163e-01, -0.0000e+00,  1.8100e-01,  1.0677e-01,  1.4056e-01],\n",
            "          [-6.2171e-02, -1.6391e-01, -1.5709e-01, -0.0000e+00,  8.7996e-02],\n",
            "          [ 1.9245e-01,  0.0000e+00, -0.0000e+00,  1.1260e-01,  0.0000e+00]]]],\n",
            "       device='cuda:0', grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(module.weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT5ae96RqxOi"
      },
      "source": [
        "最后，在每次前向传递之前应用剪枝，使用 PyTorch 的 `forward_pre_hooks`。具体来说，当 `module` 被剪枝时，正如我们所做的，它将为与之关联的每个被剪枝的参数获取一个 `forward_pre_hook`。在这种情况下，由于到目前为止我们仅剪枝了名为 `weight` 的原始参数，因此只会存在一个钩子。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "DSoHehdfqxOi",
        "outputId": "4f59507c-c129-442d-8338-590b06e6a424",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([(0, <torch.nn.utils.prune.RandomUnstructured object at 0x7f1011dcca30>)])\n"
          ]
        }
      ],
      "source": [
        "print(module._forward_pre_hooks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jEuq8ANqxOi"
      },
      "source": [
        "为了完整起见，我们现在也可以修剪 `bias`，以查看 `module` 的参数、缓冲区、钩子和属性是如何变化的。为了尝试另一种修剪技术，这里我们通过 L1 范数修剪了 `bias` 中的 3 个最小条目，具体实现见 `l1_unstructured` 修剪函数。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "f73-RcsCqxOi",
        "outputId": "1737d218-b841-4be1-cdc9-f3a6d30d1764",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "prune.l1_unstructured(module, name=\"bias\", amount=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K38G69R2qxOi"
      },
      "source": [
        "我们现在期望命名参数包括 `weight_orig`（之前的）和 `bias_orig`。缓冲区将包括 `weight_mask` 和 `bias_mask`。这两个张量的剪枝版本将作为模块属性存在，并且模块现在将有两个 `forward_pre_hooks`。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "5H6o_m6hqxOi",
        "outputId": "42221381-d0b6-460f-93d6-a0f603f86678",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('weight_orig', Parameter containing:\n",
            "tensor([[[[ 5.3230e-02, -1.9184e-01, -1.5453e-01, -1.2699e-01,  6.6093e-02],\n",
            "          [ 1.8400e-01, -9.0329e-02, -1.2064e-01, -1.8656e-01, -1.7463e-01],\n",
            "          [ 7.2806e-02, -1.5050e-01, -7.3161e-02, -1.1542e-01, -1.0708e-01],\n",
            "          [ 2.7203e-02, -1.6833e-01, -6.0692e-02, -4.8347e-02, -3.6435e-02],\n",
            "          [-1.1822e-01,  7.2270e-02,  1.9358e-01, -1.2367e-01, -1.2044e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.1181e-01, -3.6317e-03, -3.7799e-02, -2.1182e-02, -1.9294e-01],\n",
            "          [-9.4081e-02, -1.6163e-01, -1.7939e-01, -1.9383e-02,  1.7369e-01],\n",
            "          [ 1.8170e-01,  6.0109e-02, -2.9164e-02,  1.2290e-01,  4.4267e-02],\n",
            "          [-6.1668e-02, -1.8145e-01, -1.8272e-01, -4.7278e-02, -2.5070e-02],\n",
            "          [-1.5373e-01,  8.0089e-02,  2.5887e-02,  6.4049e-02,  7.2076e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.7609e-02, -1.3780e-01, -5.4621e-02, -1.2206e-02,  5.2841e-02],\n",
            "          [-5.2555e-02, -1.8912e-01,  6.6217e-02,  2.7229e-02,  2.3559e-03],\n",
            "          [-5.6309e-02, -1.8930e-01,  9.6571e-02, -4.4901e-02, -1.5537e-02],\n",
            "          [ 7.9977e-02, -5.8682e-02, -1.6047e-02, -1.0605e-01,  7.9221e-02],\n",
            "          [-1.6149e-01,  6.6729e-02, -2.9748e-02, -7.2269e-02, -1.1112e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 8.5882e-02,  1.0928e-01, -1.1594e-01, -7.2526e-02, -1.9875e-01],\n",
            "          [ 7.2294e-02, -1.0453e-01,  6.0929e-03,  1.5808e-01,  4.5754e-02],\n",
            "          [-9.5228e-03,  3.0368e-02, -1.0535e-01,  1.2455e-01, -1.0528e-01],\n",
            "          [ 1.2656e-01,  1.0943e-05, -2.4319e-02,  1.9900e-01, -6.5929e-02],\n",
            "          [ 1.7295e-01, -4.3150e-02, -1.3977e-01, -1.1677e-01,  4.0837e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.8692e-02,  5.5041e-02, -4.8174e-02, -3.2240e-02, -6.6495e-03],\n",
            "          [-1.5058e-01,  2.0878e-02,  1.2877e-01, -4.8080e-02, -1.8864e-01],\n",
            "          [ 2.4116e-02,  2.9690e-03, -8.6705e-02,  1.0196e-01, -1.5521e-01],\n",
            "          [-1.2146e-01,  3.5081e-02, -1.8921e-01, -1.6350e-02, -9.8950e-02],\n",
            "          [-1.6135e-01, -1.3056e-01,  1.2149e-01,  3.3998e-02, -1.0888e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 3.1408e-02,  1.8401e-01, -1.4511e-01, -1.9848e-01, -1.9693e-01],\n",
            "          [-2.5736e-03, -1.9103e-01,  4.6310e-02, -5.2954e-02,  1.7836e-01],\n",
            "          [-1.6163e-01, -6.6080e-02,  1.8100e-01,  1.0677e-01,  1.4056e-01],\n",
            "          [-6.2171e-02, -1.6391e-01, -1.5709e-01, -9.8989e-02,  8.7996e-02],\n",
            "          [ 1.9245e-01,  4.7125e-02, -4.9759e-02,  1.1260e-01,  4.7297e-04]]]],\n",
            "       device='cuda:0', requires_grad=True)), ('bias_orig', Parameter containing:\n",
            "tensor([-0.0082, -0.1869, -0.0693,  0.1968, -0.1167,  0.0080], device='cuda:0',\n",
            "       requires_grad=True))]\n"
          ]
        }
      ],
      "source": [
        "print(list(module.named_parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "iFx4DPBTqxOi",
        "outputId": "9d838f98-110b-4915-b663-3b9209d62a3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('weight_mask', tensor([[[[1., 0., 1., 1., 0.],\n",
            "          [1., 0., 1., 1., 0.],\n",
            "          [0., 1., 0., 1., 1.],\n",
            "          [1., 0., 0., 1., 1.],\n",
            "          [1., 0., 0., 0., 1.]]],\n",
            "\n",
            "\n",
            "        [[[1., 0., 1., 1., 1.],\n",
            "          [1., 1., 0., 1., 1.],\n",
            "          [0., 0., 1., 1., 0.],\n",
            "          [0., 0., 1., 1., 1.],\n",
            "          [0., 1., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[0., 1., 1., 1., 0.],\n",
            "          [1., 1., 1., 1., 1.],\n",
            "          [1., 0., 1., 0., 0.],\n",
            "          [1., 1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1., 0.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1., 1., 0., 1.],\n",
            "          [0., 1., 1., 0., 1.],\n",
            "          [0., 1., 0., 1., 0.],\n",
            "          [0., 1., 1., 0., 1.],\n",
            "          [1., 1., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1., 1., 1., 0.],\n",
            "          [1., 1., 1., 1., 0.],\n",
            "          [1., 1., 1., 1., 1.],\n",
            "          [0., 1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 1., 1., 0., 1.],\n",
            "          [1., 1., 0., 1., 1.],\n",
            "          [1., 0., 1., 1., 1.],\n",
            "          [1., 1., 1., 0., 1.],\n",
            "          [1., 0., 0., 1., 0.]]]], device='cuda:0')), ('bias_mask', tensor([0., 1., 0., 1., 1., 0.], device='cuda:0'))]\n"
          ]
        }
      ],
      "source": [
        "print(list(module.named_buffers()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5Pek2wSrqxOi",
        "outputId": "56a78c42-f725-491a-b162-05aa923e5a7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.0000, -0.1869, -0.0000,  0.1968, -0.1167,  0.0000], device='cuda:0',\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(module.bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Fr-azLWAqxOi",
        "outputId": "d8be6f22-6e91-4057-ea9c-2a4ab95acfc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([(0, <torch.nn.utils.prune.RandomUnstructured object at 0x7f1011dcca30>), (1, <torch.nn.utils.prune.L1Unstructured object at 0x7f1011dcf1c0>)])\n"
          ]
        }
      ],
      "source": [
        "print(module._forward_pre_hooks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hJjbs0lqxOi"
      },
      "source": [
        "迭代剪枝  \n",
        "=================\n",
        "\n",
        "同一个模块中的参数可以被多次剪枝，各次剪枝调用的效果等同于依次应用的多个掩码的组合。新掩码与旧掩码的组合由 `PruningContainer` 的 `compute_mask` 方法处理。\n",
        "\n",
        "例如，假设我们现在希望进一步剪枝 `module.weight`，这次使用基于张量第0轴的结构化剪枝（第0轴对应卷积层的输出通道，对于 `conv1`，其维度为6），剪枝依据通道的L2范数。这可以通过 `ln_structured` 函数实现，其中 `n=2` 且 `dim=0`。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "i_IfDuMRqxOi",
        "outputId": "9803677a-aba5-4389-bb92-68bcf0b9340b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[ 0.0532, -0.0000, -0.1545, -0.1270,  0.0000],\n",
            "          [ 0.1840, -0.0000, -0.1206, -0.1866, -0.0000],\n",
            "          [ 0.0000, -0.1505, -0.0000, -0.1154, -0.1071],\n",
            "          [ 0.0272, -0.0000, -0.0000, -0.0483, -0.0364],\n",
            "          [-0.1182,  0.0000,  0.0000, -0.0000, -0.1204]]],\n",
            "\n",
            "\n",
            "        [[[-0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n",
            "          [-0.0000, -0.0000, -0.0000, -0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000, -0.0000,  0.0000,  0.0000],\n",
            "          [-0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n",
            "          [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000, -0.0000, -0.0000, -0.0000,  0.0000],\n",
            "          [-0.0000, -0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [-0.0000, -0.0000,  0.0000, -0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0000, -0.0000, -0.0000,  0.0000],\n",
            "          [-0.0000,  0.0000, -0.0000, -0.0000, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [-0.0000,  0.0000, -0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000,  0.0000, -0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0000, -0.0000, -0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0387,  0.0550, -0.0482, -0.0322, -0.0000],\n",
            "          [-0.1506,  0.0209,  0.1288, -0.0481, -0.0000],\n",
            "          [ 0.0241,  0.0030, -0.0867,  0.1020, -0.1552],\n",
            "          [-0.0000,  0.0351, -0.1892, -0.0163, -0.0989],\n",
            "          [-0.1614, -0.1306,  0.1215,  0.0340, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000,  0.1840, -0.1451, -0.0000, -0.1969],\n",
            "          [-0.0026, -0.1910,  0.0000, -0.0530,  0.1784],\n",
            "          [-0.1616, -0.0000,  0.1810,  0.1068,  0.1406],\n",
            "          [-0.0622, -0.1639, -0.1571, -0.0000,  0.0880],\n",
            "          [ 0.1924,  0.0000, -0.0000,  0.1126,  0.0000]]]], device='cuda:0',\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "prune.ln_structured(module, name=\"weight\", amount=0.5, n=2, dim=0)\n",
        "\n",
        "# 正如我们所验证的，这将使对应于50%（6个通道中的3个）的所有连接归零, 同时保留先前掩码的作用。\n",
        "print(module.weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4oaposKqxOj"
      },
      "source": [
        "对应的钩子现在将是类型 `torch.nn.utils.prune.PruningContainer`，并将存储应用于 `weight` 参数的剪枝历史。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "4A4V6VF-qxOj",
        "outputId": "97cd89dc-dfab-4f48-c744-f24ca0815c42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<torch.nn.utils.prune.RandomUnstructured object at 0x7f1011dcca30>, <torch.nn.utils.prune.LnStructured object at 0x7f1011dcf610>]\n"
          ]
        }
      ],
      "source": [
        "for hook in module._forward_pre_hooks.values():\n",
        "    if hook._tensor_name == \"weight\":  # 选择正确的钩子\n",
        "        break\n",
        "\n",
        "print(list(hook))  # 在容器中的剪枝历史"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9c5C65tqxOj"
      },
      "source": [
        "序列化修剪后的模型\n",
        "==========================\n",
        "\n",
        "所有相关的张量，包括掩码缓冲区和用于计算修剪张量的原始参数都存储在模型的`state_dict`中，因此可以很容易地序列化并保存（如果需要的话）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "a-l9s38EqxOj",
        "outputId": "c9bd7bde-bbc6-4252-aba2-0bf1cf925ace",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['conv1.weight_orig', 'conv1.bias_orig', 'conv1.weight_mask', 'conv1.bias_mask', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n"
          ]
        }
      ],
      "source": [
        "print(model.state_dict().keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADAxbQMoqxOj"
      },
      "source": [
        "移除剪枝重参数化\n",
        "=================\n",
        "\n",
        "为了使剪枝永久生效，移除关于 `weight_orig` 和 `weight_mask` 的重参数化，并移除 `forward_pre_hook`，我们可以使用 `torch.nn.utils.prune` 中的 `remove` 功能。请注意，这并不会撤销剪枝，好像它从未发生过。相反，它通过将参数 `weight` 重新分配给模型参数的剪枝版本，使其永久化。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAdjFtDtqxOj"
      },
      "source": [
        "在去除重新参数化之前："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "h4t60jQvqxOj",
        "outputId": "84d2a39c-2187-436c-e18a-34a72cec8d4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('weight_orig', Parameter containing:\n",
            "tensor([[[[ 5.3230e-02, -1.9184e-01, -1.5453e-01, -1.2699e-01,  6.6093e-02],\n",
            "          [ 1.8400e-01, -9.0329e-02, -1.2064e-01, -1.8656e-01, -1.7463e-01],\n",
            "          [ 7.2806e-02, -1.5050e-01, -7.3161e-02, -1.1542e-01, -1.0708e-01],\n",
            "          [ 2.7203e-02, -1.6833e-01, -6.0692e-02, -4.8347e-02, -3.6435e-02],\n",
            "          [-1.1822e-01,  7.2270e-02,  1.9358e-01, -1.2367e-01, -1.2044e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.1181e-01, -3.6317e-03, -3.7799e-02, -2.1182e-02, -1.9294e-01],\n",
            "          [-9.4081e-02, -1.6163e-01, -1.7939e-01, -1.9383e-02,  1.7369e-01],\n",
            "          [ 1.8170e-01,  6.0109e-02, -2.9164e-02,  1.2290e-01,  4.4267e-02],\n",
            "          [-6.1668e-02, -1.8145e-01, -1.8272e-01, -4.7278e-02, -2.5070e-02],\n",
            "          [-1.5373e-01,  8.0089e-02,  2.5887e-02,  6.4049e-02,  7.2076e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.7609e-02, -1.3780e-01, -5.4621e-02, -1.2206e-02,  5.2841e-02],\n",
            "          [-5.2555e-02, -1.8912e-01,  6.6217e-02,  2.7229e-02,  2.3559e-03],\n",
            "          [-5.6309e-02, -1.8930e-01,  9.6571e-02, -4.4901e-02, -1.5537e-02],\n",
            "          [ 7.9977e-02, -5.8682e-02, -1.6047e-02, -1.0605e-01,  7.9221e-02],\n",
            "          [-1.6149e-01,  6.6729e-02, -2.9748e-02, -7.2269e-02, -1.1112e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 8.5882e-02,  1.0928e-01, -1.1594e-01, -7.2526e-02, -1.9875e-01],\n",
            "          [ 7.2294e-02, -1.0453e-01,  6.0929e-03,  1.5808e-01,  4.5754e-02],\n",
            "          [-9.5228e-03,  3.0368e-02, -1.0535e-01,  1.2455e-01, -1.0528e-01],\n",
            "          [ 1.2656e-01,  1.0943e-05, -2.4319e-02,  1.9900e-01, -6.5929e-02],\n",
            "          [ 1.7295e-01, -4.3150e-02, -1.3977e-01, -1.1677e-01,  4.0837e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.8692e-02,  5.5041e-02, -4.8174e-02, -3.2240e-02, -6.6495e-03],\n",
            "          [-1.5058e-01,  2.0878e-02,  1.2877e-01, -4.8080e-02, -1.8864e-01],\n",
            "          [ 2.4116e-02,  2.9690e-03, -8.6705e-02,  1.0196e-01, -1.5521e-01],\n",
            "          [-1.2146e-01,  3.5081e-02, -1.8921e-01, -1.6350e-02, -9.8950e-02],\n",
            "          [-1.6135e-01, -1.3056e-01,  1.2149e-01,  3.3998e-02, -1.0888e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 3.1408e-02,  1.8401e-01, -1.4511e-01, -1.9848e-01, -1.9693e-01],\n",
            "          [-2.5736e-03, -1.9103e-01,  4.6310e-02, -5.2954e-02,  1.7836e-01],\n",
            "          [-1.6163e-01, -6.6080e-02,  1.8100e-01,  1.0677e-01,  1.4056e-01],\n",
            "          [-6.2171e-02, -1.6391e-01, -1.5709e-01, -9.8989e-02,  8.7996e-02],\n",
            "          [ 1.9245e-01,  4.7125e-02, -4.9759e-02,  1.1260e-01,  4.7297e-04]]]],\n",
            "       device='cuda:0', requires_grad=True)), ('bias_orig', Parameter containing:\n",
            "tensor([-0.0082, -0.1869, -0.0693,  0.1968, -0.1167,  0.0080], device='cuda:0',\n",
            "       requires_grad=True))]\n"
          ]
        }
      ],
      "source": [
        "print(list(module.named_parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "0mijYwG4qxOj",
        "outputId": "7ca90369-2e5d-4488-e954-2e86fbbdf1c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('weight_mask', tensor([[[[1., 0., 1., 1., 0.],\n",
            "          [1., 0., 1., 1., 0.],\n",
            "          [0., 1., 0., 1., 1.],\n",
            "          [1., 0., 0., 1., 1.],\n",
            "          [1., 0., 0., 0., 1.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1., 1., 1., 0.],\n",
            "          [1., 1., 1., 1., 0.],\n",
            "          [1., 1., 1., 1., 1.],\n",
            "          [0., 1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 1., 1., 0., 1.],\n",
            "          [1., 1., 0., 1., 1.],\n",
            "          [1., 0., 1., 1., 1.],\n",
            "          [1., 1., 1., 0., 1.],\n",
            "          [1., 0., 0., 1., 0.]]]], device='cuda:0')), ('bias_mask', tensor([0., 1., 0., 1., 1., 0.], device='cuda:0'))]\n"
          ]
        }
      ],
      "source": [
        "print(list(module.named_buffers()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Q7hiww54qxOj",
        "outputId": "7a0c7f7a-eb50-4856-8d20-8eea076e0a97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[ 0.0532, -0.0000, -0.1545, -0.1270,  0.0000],\n",
            "          [ 0.1840, -0.0000, -0.1206, -0.1866, -0.0000],\n",
            "          [ 0.0000, -0.1505, -0.0000, -0.1154, -0.1071],\n",
            "          [ 0.0272, -0.0000, -0.0000, -0.0483, -0.0364],\n",
            "          [-0.1182,  0.0000,  0.0000, -0.0000, -0.1204]]],\n",
            "\n",
            "\n",
            "        [[[-0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n",
            "          [-0.0000, -0.0000, -0.0000, -0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000, -0.0000,  0.0000,  0.0000],\n",
            "          [-0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n",
            "          [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000, -0.0000, -0.0000, -0.0000,  0.0000],\n",
            "          [-0.0000, -0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [-0.0000, -0.0000,  0.0000, -0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0000, -0.0000, -0.0000,  0.0000],\n",
            "          [-0.0000,  0.0000, -0.0000, -0.0000, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [-0.0000,  0.0000, -0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000,  0.0000, -0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0000, -0.0000, -0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0387,  0.0550, -0.0482, -0.0322, -0.0000],\n",
            "          [-0.1506,  0.0209,  0.1288, -0.0481, -0.0000],\n",
            "          [ 0.0241,  0.0030, -0.0867,  0.1020, -0.1552],\n",
            "          [-0.0000,  0.0351, -0.1892, -0.0163, -0.0989],\n",
            "          [-0.1614, -0.1306,  0.1215,  0.0340, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000,  0.1840, -0.1451, -0.0000, -0.1969],\n",
            "          [-0.0026, -0.1910,  0.0000, -0.0530,  0.1784],\n",
            "          [-0.1616, -0.0000,  0.1810,  0.1068,  0.1406],\n",
            "          [-0.0622, -0.1639, -0.1571, -0.0000,  0.0880],\n",
            "          [ 0.1924,  0.0000, -0.0000,  0.1126,  0.0000]]]], device='cuda:0',\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(module.weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKQa2T2nqxOj"
      },
      "source": [
        "去除重新参数化后：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "vo4IqMSFqxOj",
        "outputId": "7967fcc9-5ba2-4c9b-952a-4b3de3ff9ecb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('bias_orig', Parameter containing:\n",
            "tensor([-0.0082, -0.1869, -0.0693,  0.1968, -0.1167,  0.0080], device='cuda:0',\n",
            "       requires_grad=True)), ('weight', Parameter containing:\n",
            "tensor([[[[ 0.0532, -0.0000, -0.1545, -0.1270,  0.0000],\n",
            "          [ 0.1840, -0.0000, -0.1206, -0.1866, -0.0000],\n",
            "          [ 0.0000, -0.1505, -0.0000, -0.1154, -0.1071],\n",
            "          [ 0.0272, -0.0000, -0.0000, -0.0483, -0.0364],\n",
            "          [-0.1182,  0.0000,  0.0000, -0.0000, -0.1204]]],\n",
            "\n",
            "\n",
            "        [[[-0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n",
            "          [-0.0000, -0.0000, -0.0000, -0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000, -0.0000,  0.0000,  0.0000],\n",
            "          [-0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n",
            "          [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000, -0.0000, -0.0000, -0.0000,  0.0000],\n",
            "          [-0.0000, -0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [-0.0000, -0.0000,  0.0000, -0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0000, -0.0000, -0.0000,  0.0000],\n",
            "          [-0.0000,  0.0000, -0.0000, -0.0000, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [-0.0000,  0.0000, -0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000,  0.0000, -0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0000, -0.0000, -0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0387,  0.0550, -0.0482, -0.0322, -0.0000],\n",
            "          [-0.1506,  0.0209,  0.1288, -0.0481, -0.0000],\n",
            "          [ 0.0241,  0.0030, -0.0867,  0.1020, -0.1552],\n",
            "          [-0.0000,  0.0351, -0.1892, -0.0163, -0.0989],\n",
            "          [-0.1614, -0.1306,  0.1215,  0.0340, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000,  0.1840, -0.1451, -0.0000, -0.1969],\n",
            "          [-0.0026, -0.1910,  0.0000, -0.0530,  0.1784],\n",
            "          [-0.1616, -0.0000,  0.1810,  0.1068,  0.1406],\n",
            "          [-0.0622, -0.1639, -0.1571, -0.0000,  0.0880],\n",
            "          [ 0.1924,  0.0000, -0.0000,  0.1126,  0.0000]]]], device='cuda:0',\n",
            "       requires_grad=True))]\n"
          ]
        }
      ],
      "source": [
        "prune.remove(module, 'weight')\n",
        "print(list(module.named_parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "5VfnUFASqxOj",
        "outputId": "25fc86f9-0e3f-4e6f-e1d7-2d3cae783bb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('bias_mask', tensor([0., 1., 0., 1., 1., 0.], device='cuda:0'))]\n"
          ]
        }
      ],
      "source": [
        "print(list(module.named_buffers()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9mNzqn-qxOo"
      },
      "source": [
        "在模型中修剪多个参数\n",
        "=====================\n",
        "\n",
        "通过指定所需的修剪技术和参数，我们可以轻松地修剪网络中的多个张量，可能根据它们的类型，正如我们在这个例子中将看到的。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "IqohAFRCqxOo",
        "outputId": "4a527761-9119-4ccb-89c8-cc60c9a1189e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['conv1.weight_mask', 'conv2.weight_mask', 'fc1.weight_mask', 'fc2.weight_mask', 'fc3.weight_mask'])\n"
          ]
        }
      ],
      "source": [
        "new_model = LeNet()\n",
        "for name, module in new_model.named_modules():\n",
        "    # 在所有 2D 卷积层中修剪 20% 的连接\n",
        "    if isinstance(module, torch.nn.Conv2d):\n",
        "        prune.l1_unstructured(module, name='weight', amount=0.2)\n",
        "    # 在所有线性层中修剪 40% 的连接\n",
        "    elif isinstance(module, torch.nn.Linear):\n",
        "        prune.l1_unstructured(module, name='weight', amount=0.4)\n",
        "\n",
        "print(dict(new_model.named_buffers()).keys())  # 验证所有掩码是否存在"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m41SEsvgqxOo"
      },
      "source": [
        "全局剪枝\n",
        "==============\n",
        "\n",
        "到目前为止，我们只看到了通常所称的“局部”剪枝，即逐个修剪模型中的张量，通过将每个条目的统计数据（权重大小、激活、梯度等）与该张量中的其他条目进行比较。然而，一种常见且可能更强大的技术是一次性修剪整个模型，例如，移除整个模型中最低的20%的连接，而不是在每一层中去除最低的20%的连接。这可能导致每一层的剪枝比例不同。让我们看看如何使用 `torch.nn.utils.prune` 中的 `global_unstructured` 来实现这一点。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "APyyfr0tqxOo"
      },
      "outputs": [],
      "source": [
        "model = LeNet()\n",
        "\n",
        "# 定义需要进行剪枝的参数\n",
        "parameters_to_prune = (\n",
        "    (model.conv1, 'weight'),\n",
        "    (model.conv2, 'weight'),\n",
        "    (model.fc1, 'weight'),\n",
        "    (model.fc2, 'weight'),\n",
        "    (model.fc3, 'weight'),\n",
        ")\n",
        "\n",
        "# 全局非结构化剪枝，使用L1范数作为剪枝方法，剪枝比例为0.2\n",
        "prune.global_unstructured(\n",
        "    parameters_to_prune,\n",
        "    pruning_method=prune.L1Unstructured,  # 使用L1非结构化剪枝方法\n",
        "    amount=0.2,  # 剪枝20%的权重\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cooOWyKuqxOo"
      },
      "source": [
        "现在我们可以检查每个剪枝参数所诱导的稀疏性，这在每一层中不会等于20%。但是，整体稀疏性将是（大约）20%。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "NpGZL0EHqxOo",
        "outputId": "fe196291-d317-4a3b-b969-78cab89c00bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sparsity in conv1.weight: 8.00%\n",
            "Sparsity in conv2.weight: 13.88%\n",
            "Sparsity in fc1.weight: 22.16%\n",
            "Sparsity in fc2.weight: 12.07%\n",
            "Sparsity in fc3.weight: 11.55%\n",
            "Global sparsity: 20.00%\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    \"Sparsity in conv1.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.conv1.weight == 0))\n",
        "        / float(model.conv1.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Sparsity in conv2.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.conv2.weight == 0))\n",
        "        / float(model.conv2.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Sparsity in fc1.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.fc1.weight == 0))\n",
        "        / float(model.fc1.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Sparsity in fc2.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.fc2.weight == 0))\n",
        "        / float(model.fc2.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Sparsity in fc3.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.fc3.weight == 0))\n",
        "        / float(model.fc3.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Global sparsity: {:.2f}%\".format(\n",
        "        100. * float(\n",
        "            torch.sum(model.conv1.weight == 0)\n",
        "            + torch.sum(model.conv2.weight == 0)\n",
        "            + torch.sum(model.fc1.weight == 0)\n",
        "            + torch.sum(model.fc2.weight == 0)\n",
        "            + torch.sum(model.fc3.weight == 0)\n",
        "        )\n",
        "        / float(\n",
        "            model.conv1.weight.nelement()\n",
        "            + model.conv2.weight.nelement()\n",
        "            + model.fc1.weight.nelement()\n",
        "            + model.fc2.weight.nelement()\n",
        "            + model.fc3.weight.nelement()\n",
        "        )\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68PqGAyjqxOo"
      },
      "source": [
        "扩展 `torch.nn.utils.prune` 自定义修剪函数\n",
        "==============================================================\n",
        "\n",
        "要实现您自己的修剪函数，可以通过继承 `BasePruningMethod` 基类来扩展 `nn.utils.prune` 模块，方法与所有其他修剪方法相同。基类为您实现了以下方法：`__call__`，`apply_mask`，`apply`，`prune` 和 `remove`。除了某些特例外，您无需重新实现这些方法。不过，您需要实现 `__init__`（修剪参数的…）。\n",
        "\n",
        "假设您想实现一种修剪技术，该技术修剪张量中的每个其他条目（或者如果张量之前已被修剪，则修剪张量中剩余的未修剪部分）。这将属于 `PRUNING_TYPE='unstructured'`，因为它作用于层中的单个连接，而不是整个单元/通道（`'structured'`），或在不同的参数之间（`'global'`）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "YZeJGPfSqxOo"
      },
      "outputs": [],
      "source": [
        "class FooBarPruningMethod(prune.BasePruningMethod):\n",
        "    \"\"\"剪枝张量中每隔一个位置的元素\"\"\"\n",
        "    PRUNING_TYPE = 'unstructured'  # 非结构化剪枝\n",
        "\n",
        "    def compute_mask(self, t, default_mask):\n",
        "        # 计算掩码\n",
        "        mask = default_mask.clone()  # 克隆默认掩码\n",
        "        mask.view(-1)[::2] = 0  # 将掩码每隔一个元素置为0，实现隔位剪枝\n",
        "        return mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DTuWHDFqxOp"
      },
      "source": [
        "现在，在将其应用于`nn.Module`中的参数时，你还需要提供一个简单的函数，用于实例化该方法并应用它。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "06YTfqQCqxOp"
      },
      "outputs": [],
      "source": [
        "def foobar_unstructured(module, name):\n",
        "    \"\"\"通过移除张量中每隔一个位置的元素，对`module`中名为`name`的参数执行剪枝操作。\n",
        "    此操作会直接修改模块本身（同时返回修改后的模块），具体操作包括：\n",
        "    1) 添加一个名为`name+'_mask'`的缓冲区，对应剪枝方法应用于参数`name`的二进制掩码。\n",
        "    参数`name`会被其剪枝后的版本替代，而原始（未剪枝）的参数会存储在一个新参数\n",
        "    `name+'_orig'`中。\n",
        "\n",
        "    示例:\n",
        "        >>> m = nn.Linear(3, 4)\n",
        "        >>> foobar_unstructured(m, name='bias')\n",
        "    \"\"\"\n",
        "    FooBarPruningMethod.apply(module, name)  # 应用FooBarPruningMethod剪枝方法\n",
        "    return module  # 返回修改后的模块"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weNxxnyNqxOp"
      },
      "source": [
        "让我们试试看！\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "XYK9zmHAqxOp",
        "outputId": "ed2db209-3ea2-474d-dae8-78937d533e81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0., 1., 0., 1., 0., 1., 0., 1., 0., 1.])\n"
          ]
        }
      ],
      "source": [
        "model = LeNet()\n",
        "foobar_unstructured(model.fc3, name='bias')\n",
        "\n",
        "print(model.fc3.bias_mask)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}